{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99vLHR5SIeEL",
        "outputId": "03a303f5-accf-4117-cf80-812b20be9880"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313470 sha256=4b06f0c4dc2e7a448a9f9c6208bafe2edfd4e0652e82dc97fe869549cfda32b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/facebookresearch/fastText.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXR16XfLIicd",
        "outputId": "7afbc895-da4e-4075-a39b-ee9b31306dce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fastText.git\n",
            "  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-nac_j8_p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-nac_j8_p\n",
            "  Resolved https://github.com/facebookresearch/fastText.git to commit 1142dc4c4ecbc19cc16eee5cdd28472e689267e6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext==0.9.2) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext==0.9.2) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext==0.9.2) (2.0.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp311-cp311-linux_x86_64.whl size=4313396 sha256=5449f96162cc1cf4613931051aff4a7425b859165b222f968e2dfb1cef6704ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a841t5fw/wheels/04/64/26/11ce8db1ddfa20541eeec84e6969a9d7582367261378c65307\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "  Attempting uninstall: fasttext\n",
            "    Found existing installation: fasttext 0.9.3\n",
            "    Uninstalling fasttext-0.9.3:\n",
            "      Successfully uninstalled fasttext-0.9.3\n",
            "Successfully installed fasttext-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Input, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('filipino_resumes_with_diverse_cover_letters.csv')\n",
        "\n",
        "# Preprocessing\n",
        "data['Education'] = data['Education'].apply(lambda x: 1 if x == 'Masters' else 0)\n",
        "data['Experience'] = pd.to_numeric(data['Experience (Years)'], errors='coerce')\n",
        "data['Age'] = pd.to_numeric(data['Age'], errors='coerce')\n",
        "\n",
        "# Map roles to numeric classes (Chemical Engineer -> 0, Accountant -> 1, None -> 2)\n",
        "role_map = {'Chemical Engineer': 0, 'Accountant': 1, 'None': 2}\n",
        "data['Role'] = data['Role'].map(role_map)\n",
        "\n",
        "# One-hot encode the target variable (Role)\n",
        "y = to_categorical(data['Role'], num_classes=3)\n",
        "\n",
        "# Features: Resume data\n",
        "X_resume = data[['Age', 'Experience', 'Education']]\n",
        "\n",
        "# Train-test split\n",
        "X_train_res, X_test_res, y_train, y_test = train_test_split(X_resume, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the resume features\n",
        "scaler = StandardScaler()\n",
        "X_train_res = scaler.fit_transform(X_train_res)\n",
        "X_test_res = scaler.transform(X_test_res)\n",
        "\n",
        "# Save the scaler to a file\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Tokenizer for cover letter\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Cover Letter'])  # Ensure 'Cover Letter' column exists in your dataset\n",
        "\n",
        "# Save tokenizer for future use\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Convert cover letters to sequences\n",
        "X_cover = tokenizer.texts_to_sequences(data['Cover Letter'])\n",
        "\n",
        "# Pad sequences for consistency\n",
        "X_cover = pad_sequences(X_cover, padding='post')\n",
        "\n",
        "# Train-test split for cover letter data\n",
        "X_train_cover, X_test_cover = train_test_split(X_cover, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build the model with regularization and adjusted complexity\n",
        "resume_input = Input(shape=(X_train_res.shape[1],), name='resume_input')\n",
        "x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(resume_input)\n",
        "x = Dropout(0.6)(x)  # Increased dropout to 0.6\n",
        "x = Dense(16, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "# Cover letter input (text data)\n",
        "cover_input = Input(shape=(X_train_cover.shape[1],), name='cover_input')\n",
        "y = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100)(cover_input)\n",
        "y = LSTM(32, kernel_regularizer=l2(0.01))(y)  # Reduced LSTM units\n",
        "y = Dropout(0.6)(y)  # Increased dropout to 0.6\n",
        "y = Dense(16, activation='relu', kernel_regularizer=l2(0.01))(y)\n",
        "\n",
        "# Concatenate both inputs\n",
        "combined = concatenate([x, y])\n",
        "\n",
        "# Output layer\n",
        "z = Dense(3, activation='softmax')(combined)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model(inputs=[resume_input, cover_input], outputs=z)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Class weights to handle imbalanced data (giving more importance to minority classes)\n",
        "class_weights = {0: 2.0, 1: 1.0, 2: 2.0}\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_res, X_train_cover], y_train, epochs=100, batch_size=32, verbose=1,\n",
        "          class_weight=class_weights, validation_data=([X_test_res, X_test_cover], y_test),\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "# Save the model as 'model.h5'\n",
        "model.save('model.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict([X_test_res, X_test_cover])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert y_test back to single integer values (not one-hot)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Display classification report and accuracy\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test_classes, y_pred_classes))\n",
        "\n",
        "# Save label encoders for roles\n",
        "role_encoder = LabelEncoder()\n",
        "role_encoder.fit(data['Role'])\n",
        "with open('label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(role_encoder, f)\n",
        "\n",
        "print(\"Model, scalers, and encoders saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn6f_UVJcNJF",
        "outputId": "e75531d6-b69e-4ad9-c151-40b9056765d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.4261 - loss: 2.6930 - val_accuracy: 0.4883 - val_loss: 1.6784\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.6238 - loss: 1.5891 - val_accuracy: 1.0000 - val_loss: 0.7030\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9955 - loss: 0.6804 - val_accuracy: 1.0000 - val_loss: 0.4109\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9847 - loss: 0.5083 - val_accuracy: 1.0000 - val_loss: 0.3340\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.9991 - loss: 0.3511 - val_accuracy: 1.0000 - val_loss: 0.2762\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9996 - loss: 0.2874 - val_accuracy: 1.0000 - val_loss: 0.2327\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.2458 - val_accuracy: 1.0000 - val_loss: 0.1984\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.9997 - loss: 0.2082 - val_accuracy: 1.0000 - val_loss: 0.1702\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1774 - val_accuracy: 1.0000 - val_loss: 0.1469\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1533 - val_accuracy: 1.0000 - val_loss: 0.1277\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1351 - val_accuracy: 1.0000 - val_loss: 0.1119\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.1192 - val_accuracy: 1.0000 - val_loss: 0.0986\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.9998 - loss: 0.1055 - val_accuracy: 1.0000 - val_loss: 0.0876\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0945 - val_accuracy: 1.0000 - val_loss: 0.0784\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9997 - loss: 0.0865 - val_accuracy: 1.0000 - val_loss: 0.0706\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0772 - val_accuracy: 1.0000 - val_loss: 0.0641\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.9999 - loss: 0.0707 - val_accuracy: 1.0000 - val_loss: 0.0585\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0652 - val_accuracy: 1.0000 - val_loss: 0.0536\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0604 - val_accuracy: 1.0000 - val_loss: 0.0497\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0560 - val_accuracy: 1.0000 - val_loss: 0.0457\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0509 - val_accuracy: 1.0000 - val_loss: 0.0427\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0476 - val_accuracy: 1.0000 - val_loss: 0.0398\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0450 - val_accuracy: 1.0000 - val_loss: 0.0375\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0419 - val_accuracy: 1.0000 - val_loss: 0.0351\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.9997 - loss: 0.0403 - val_accuracy: 1.0000 - val_loss: 0.0335\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0388 - val_accuracy: 1.0000 - val_loss: 0.0314\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0353 - val_accuracy: 1.0000 - val_loss: 0.0297\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0341 - val_accuracy: 1.0000 - val_loss: 0.0280\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0322 - val_accuracy: 1.0000 - val_loss: 0.0264\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0302 - val_accuracy: 1.0000 - val_loss: 0.0252\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 1.0000 - val_loss: 0.0237\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0286 - val_accuracy: 1.0000 - val_loss: 0.0227\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0274 - val_accuracy: 1.0000 - val_loss: 0.0216\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0246 - val_accuracy: 1.0000 - val_loss: 0.0203\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0236 - val_accuracy: 1.0000 - val_loss: 0.0193\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0239 - val_accuracy: 1.0000 - val_loss: 0.0186\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0230 - val_accuracy: 1.0000 - val_loss: 0.0178\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0202 - val_accuracy: 1.0000 - val_loss: 0.0168\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0194 - val_accuracy: 1.0000 - val_loss: 0.0161\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0182 - val_accuracy: 1.0000 - val_loss: 0.0150\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0172 - val_accuracy: 1.0000 - val_loss: 0.0142\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0161 - val_accuracy: 1.0000 - val_loss: 0.0136\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0155 - val_accuracy: 1.0000 - val_loss: 0.0128\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0156 - val_accuracy: 1.0000 - val_loss: 0.0125\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0144 - val_accuracy: 1.0000 - val_loss: 0.0116\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0142 - val_accuracy: 1.0000 - val_loss: 0.0111\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0134 - val_accuracy: 1.0000 - val_loss: 0.0106\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 1.0000 - val_loss: 0.0102\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 0.0097\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 1.0000 - val_loss: 0.0093\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0107 - val_accuracy: 1.0000 - val_loss: 0.0088\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 1.0000 - val_loss: 0.0082\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0112 - val_accuracy: 1.0000 - val_loss: 0.0081\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 0.0072\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 0.0062\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 0.0059\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0056\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0054\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0052\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0050\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 0.0050\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0047\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0041\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0041\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9983 - loss: 0.0216 - val_accuracy: 0.5117 - val_loss: 0.8003\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5124 - loss: 1.0523 - val_accuracy: 0.6492 - val_loss: 0.6191\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.6648 - loss: 0.8119 - val_accuracy: 0.8900 - val_loss: 0.4562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       586\n",
            "           1       1.00      1.00      1.00       614\n",
            "\n",
            "    accuracy                           1.00      1200\n",
            "   macro avg       1.00      1.00      1.00      1200\n",
            "weighted avg       1.00      1.00      1.00      1200\n",
            "\n",
            "Accuracy Score: 1.0\n",
            "Model, scalers, and encoders saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "model.save(\"model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "ElA3Gk-sj0sX",
        "outputId": "85280797-2b67-43dc-b41c-5f41b423e4b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-348ae035c5c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the entire model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming label_encoders is a dictionary of LabelEncoders used for categorical columns\n",
        "with open(\"label_encoders.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoders, f)"
      ],
      "metadata": {
        "id": "RHug4pzpkaMR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CfNuNv6uNLN",
        "outputId": "994532af-b863-4faa-ba30-3171c4e99899"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import secrets\n",
        "print(secrets.token_urlsafe(24))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSLd5ueRuiZT",
        "outputId": "37c59d11-a410-4ddf-f6c2-fa9cc6b291a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9J8-Yj5_J989bv25dKiGGYNO1SuMxyQU\n"
          ]
        }
      ]
    }
  ]
}